[{"path":"index.html","id":"résumés","chapter":"Résumés","heading":"Résumés","text":"RésuméLes moyennes mobiles (ou les filtres linéaires) sont omniprésents dans les méthodes d’extraction du cycle économique.\nAu centre de la série, des filtres symétriques sont appliqués.\nCependant, en raison du manque d’observations futures, les estimations en temps réel doivent s’appuyer sur des moyennes mobiles asymétriques.\nLes moyennes mobiles asymétriques classiques minimisent les erreurs de révision mais introduisent des retards dans la détection des points de retournement (déphasage).Ce rapport décrit et compare différentes approches pour construire des filtres asymétriques: l’ajustement de polynôme locaux, des méthodes basées sur une optimisation des propriétés des filtres (l’approche Fidelity-Smoothness-Timeliness — Fidélité-Lissage-Temporalité —, FST, approche et un filtre dépendant des données) et des filtres basés sur les espaces de Hilbert à noyau reproduisant (RKHS).\nIl décrit également comment les filtres polynomiaux locaux peuvent être étendus pour inclure un critère de temporalité afin de minimiser le déphasage.Ce rapport montre que contraindre les filtres asymétriques à ne conserver que les tendances constantes et linéaires (et pas nécessairement polynomiales) réduit l’erreur de révision et le décalage temporel.\nPar ailleurs, plus d’observations futures sont disponibles, plus les différentes méthodes produisent des résultats similaires.\nEnfin, l’approche FST semble l’emporter sur les autres méthodes.AbstractIn business cycle analysis, estimates usually derived moving average (also called linear filters) techniques.\ncenter series, symmetric filters applied.\nHowever, due lack future observations, real-time estimates must rely asymmetric moving averages.\nClassic asymmetric moving averages minimize revisions errors introduce delays detecting turning points.paper describes compares different approaches build asymmetric filters: local polynomials filters, methods based optimization filters’ properties (Fidelity-Smoothness-Timeliness, FST, approach data-dependent filter) filters based Reproducing Kernel Hilbert Space.\nalso describes local polynomials filters can extended include timeliness criterion minimize phase shift.paper shows constraining asymmetric filters preserve constant linear trends (necessarily polynomial ones) reduce revision error time lag.\nBesides, future observations available, different methods produce similar results.\nFinally, FST approach seems outperform methods.","code":""},{"path":"intro.html","id":"intro","chapter":"Chapitre 1 Introduction","heading":"Chapitre 1 Introduction","text":"","code":""},{"path":"sec:SAtoTCE.html","id":"sec:SAtoTCE","chapter":"Chapitre 2 De la désaisonnalisation à l’estimation de la tendance-cycle","heading":"Chapitre 2 De la désaisonnalisation à l’estimation de la tendance-cycle","text":"infra-annual statistics produced, especially national institutes, analyze short-term evolution economies.\nexample case gross domestic product (GDP), unemployment rate, household consumption goods industrial production indices.\nHowever, time series affected seasonal trading days effects.\nseasonal effect effect occurs calendar month similar magnitude direction year year.\ninstance, automobile production usually lower summer, due holidays, chocolate sales usually higher December, due Christmas.\nTrading days effect appears time series affected calendar month’s weekday composition.\nexample, retail sales usually higher Saturday, thus likely higher months surplus weekend days.Seasonal trading days effects can hamper analysis infra-annual movements time series spatial comparison.\ntime series often seasonally trading days adjusted, seasonal adjustment process removing effects seasonal trading day fluctuations.perform seasonal adjustment, algorithm decompose data several unknown components: trend-cycle, seasonal irregular component.\nX-12ARIMA TRAMO-SEATS (popular seasonal adjustment methods), prior decomposition, initial series pre-adjustment deterministic effects (outliers, calendar effects).\nThus, estimation trend-cycle component technically linked seasonal component.Moreover, since trend-cycle extraction methods applied seasonally adjusted data, problems separated.\nlink also explains paper, methods used implemented core libraries JDemetra+,1 seasonal adjustment software recommended Eurostat.\n interface implemented package rjdfilters2 developed internship.literature, different approaches considered trend-cycle extraction3.\nAmong popular, can cite Model-Based Approach non-parametric extraction methods:Model-Based Approach assumes specification stochastic time series model trend (ARIMA model, state space model, etc.) estimates obtained minimizing penalty function, generally mean squared error.\nexample case Kalman filter, Wiener-Kolmogorov filter (used TRAMO-SEATS) Direct Filter Approach Wildi et McElroy (2019) (section 7).Model-Based Approach assumes specification stochastic time series model trend (ARIMA model, state space model, etc.) estimates obtained minimizing penalty function, generally mean squared error.\nexample case Kalman filter, Wiener-Kolmogorov filter (used TRAMO-SEATS) Direct Filter Approach Wildi et McElroy (2019) (section 7).non-parametric extraction methods assume structure model fixed can easily applied time series.\nexample case Henderson Musgrave filters (used X-12ARIMA).\nClassical methods can seen local polynomial regression, approach generalized Proietti et Luati (2008) (section ??).\nNon-parametric estimators can also reproduced exploiting Reproducing Kernel Hilbert Space (RKHS) methodology, like done \nDagum et Bianconcini (2008) (section 6).non-parametric extraction methods assume structure model fixed can easily applied time series.\nexample case Henderson Musgrave filters (used X-12ARIMA).\nClassical methods can seen local polynomial regression, approach generalized Proietti et Luati (2008) (section ??).\nNon-parametric estimators can also reproduced exploiting Reproducing Kernel Hilbert Space (RKHS) methodology, like done \nDagum et Bianconcini (2008) (section 6).Grun-Rehomme, Guggemos, et Ladiray (2018) (section 4.2) drawn general unifying approach allows theoretical link Musgrave non-parametric filter Direct Filter Approach.\nalso proposed global procedure build asymmetric moving averages, allows minimize phase shift effects described report.paper focus methods implemented X-12ARIMA.\nmaintain consistency non-parametric approach X-12ARIMA, focus non-parametric extraction methods.\n’s neither filters approach Wildi et McElroy (2019) (section 7), model based approaches used report.Furthermore, simplicity data used seasonally adjusted: study extended see impact overall seasonal adjustment process.","code":""},{"path":"sec:propMM.html","id":"sec:propMM","chapter":"Chapitre 3 Moving average and filters","heading":"Chapitre 3 Moving average and filters","text":"Cette section décrit des définitions et les propriétés des moyennes mobiles (voir par exemple Ladiray (2018) pour plus de détails).Soient deux entiers \\(p\\) et \\(f\\). Une moyenne mobile \\(M_\\theta\\) ou \\(M\\) est un opérateur linéaire définit par un ensemble de coefficients \\(\\theta=(\\theta_{-p},\\dots,\\theta_{f})'\\) qui transforme toute série temporelle \\(X_t\\) en :\n\\[\nM_\\theta(X_t)=\\sum_{k=-p}^{+f}\\theta_kX_{t+k}.\n\\]\\(p+f+1\\) est appelée ordre de la moyenne mobile.\\(p+f+1\\) est appelée ordre de la moyenne mobile.Lorsque \\(p=f\\) la moyenne mobile est dite centrée.\nSi de plus \\(\\forall k:\\:\\theta_{-k} = \\theta_k\\), la moyenne mobile \\(M_\\theta\\) est dite symétrique.\nDans ce cas, la quantité \\(h=p=f\\) est appelée fenêtre (bandwidth).Lorsque \\(p=f\\) la moyenne mobile est dite centrée.\nSi de plus \\(\\forall k:\\:\\theta_{-k} = \\theta_k\\), la moyenne mobile \\(M_\\theta\\) est dite symétrique.\nDans ce cas, la quantité \\(h=p=f\\) est appelée fenêtre (bandwidth).","code":""},{"path":"sec:propMM.html","id":"gain-et-fonction-de-déphasage","chapter":"Chapitre 3 Moving average and filters","heading":"3.1 Gain et fonction de déphasage","text":"Soit \\(X_t=\\e^{-\\omega t}\\). La moyenne mobile \\(M_\\theta\\) transforme \\(X_t\\) en :\n\\[\nY_t = M_{\\theta}X_t = \\sum_{k=-p}^{+f} \\theta_k \\e^{-\\omega (t+k)}\n= \\left(\\sum_{k=-p}^{+f} \\theta_k \\e^{-\\omega k}\\right)\\cdot X_t.\n\\]\nLa fonction \\(\\Gamma_\\theta(\\omega)=\\sum_{k=-p}^{+f} \\theta_k e^{-\\omega k}\\) est appelée fonction de transfert ou fonction de réponse en fréquence (frequency response function)4.\nElle peut être réécrite en :\n\\[\n\\Gamma_\\theta(\\omega) = G_\\theta(\\omega)\\e^{-\\Phi_\\theta(\\omega)},\n\\]\noù \\(G_\\theta(\\omega)=\\lvert\\Gamma_\\theta(\\omega)\\rvert\\) est la fonction de gain ou d’amplitude et \\(\\Phi_\\theta(\\omega)\\) est le déphasage (phase shift ou time shift)5.\nPour tous les filtres symétriques \\(\\Phi_\\theta(\\omega)\\equiv 0 \\pmod{\\pi}\\).En somme, l’application d’une moyenne mobile à une série harmonique la modifie de deux façons :en la multipliant par un coefficient égal à \\(G_{\\theta}\\left(\\omega\\right)\\) ;en la multipliant par un coefficient égal à \\(G_{\\theta}\\left(\\omega\\right)\\) ;en la “décalant” dans le temps de \\(\\Phi_\\theta(\\omega)/\\omega\\), ce qui un impact sur la détection des points de retournement6.en la “décalant” dans le temps de \\(\\Phi_\\theta(\\omega)/\\omega\\), ce qui un impact sur la détection des points de retournement6.Par exemple, avec \\(M_{\\theta_0}X_t=\\frac{1}{2}X_{t-1}+\\frac{1}{2}X_{t}\\) :\n\\[\n\\Gamma_{\\theta_0}(\\omega)=\\frac{1}{2}+\\frac{1}{2}\\e^{-\\omega}\n=\\lvert\\cos(\\omega/2)\\rvert\\e^{-\\frac{\\omega}{2}}\n\\]\nLa figure 3.1 trace le gain et le déphasage pour \\(\\omega=\\pi/2\\) \\(X_t=\\sin(\\omega t)\\).\nFigure 3.1 : Smoothing time series \\(X_t=\\sin(\\omega t)\\) moving average \\(M_{\\theta_0}X_t=\\frac{1}{2}X_{t-1}+\\frac{1}{2}X_{t}\\) \\(\\omega=\\pi/2\\).\n","code":""},{"path":"sec:propMM.html","id":"propriétés-souhaitables-dune-moyenne-mobile","chapter":"Chapitre 3 Moving average and filters","heading":"3.2 Propriétés souhaitables d’une moyenne mobile","text":"Pour décomposer une série temporelle en une composante saisonnière, une tendance-cyle et l’irrégulier, l’algorithme de décomposition X-11 utilise une succession de moyennes mobiles ayant toutes des contraintes spécifiques. Dans cette sous-section nous décrivons trois types de contraintes :la préservation de certaines tendances ;la préservation de certaines tendances ;suppression de la saisonnalité ;suppression de la saisonnalité ;la réduction du bruit.la réduction du bruit.","code":""},{"path":"sec:propMM.html","id":"préservation-de-tendances","chapter":"Chapitre 3 Moving average and filters","heading":"3.2.1 Préservation de tendances","text":"Il est souvent souhaitable qu’une moyenne mobile conserve certaines tendances.\nUne moyenne mobile \\(M_\\theta\\) conserve une fonction du temps \\(f(t)\\) si \\(\\forall t:\\:M_\\theta f(t)=f(t)\\).Nous avons les propriétés suivantes pour la moyenne mobile \\(M_\\theta\\) :Pour conserver les constantes \\(X_t=\\) il faut que\n\\[\n\\forall t:M_\\theta(X_t)=\\sum_{k=-p}^{+f}\\theta_kX_{t+k}=\\sum_{k=-p}^{+f}\\theta_ka=\\sum_{k=-p}^{+f}\\theta_k=.\n\\]\nC’est-à-dire qu’il faut que la somme des coefficients \\(\\sum_{k=-p}^{+f}\\theta_k\\) soit égale à \\(1\\).Pour conserver les constantes \\(X_t=\\) il faut que\n\\[\n\\forall t:M_\\theta(X_t)=\\sum_{k=-p}^{+f}\\theta_kX_{t+k}=\\sum_{k=-p}^{+f}\\theta_ka=\\sum_{k=-p}^{+f}\\theta_k=.\n\\]\nC’est-à-dire qu’il faut que la somme des coefficients \\(\\sum_{k=-p}^{+f}\\theta_k\\) soit égale à \\(1\\).Pour conserver les tendances linéaires \\(X_t=+b\\) il faut que :\n\\[\n\\forall t:\\:M_\\theta(X_t)=\\sum_{k=-p}^{+f}\\theta_kX_{t+k}=\\sum_{k=-p}^{+f}\\theta_k[(t+k)+b]=\\sum_{k=-p}^{+f}k\\theta_k+b\\sum_{k=-p}^{+f}\\theta_k=+b.\n\\]\nCe qui est équivalent à :\n\\[\n\\sum_{k=-p}^{+f}\\theta_k=1\n\\quad\\text{}\\quad\n\\sum_{k=-p}^{+f}k\\theta_k=0.\n\\]Pour conserver les tendances linéaires \\(X_t=+b\\) il faut que :\n\\[\n\\forall t:\\:M_\\theta(X_t)=\\sum_{k=-p}^{+f}\\theta_kX_{t+k}=\\sum_{k=-p}^{+f}\\theta_k[(t+k)+b]=\\sum_{k=-p}^{+f}k\\theta_k+b\\sum_{k=-p}^{+f}\\theta_k=+b.\n\\]\nCe qui est équivalent à :\n\\[\n\\sum_{k=-p}^{+f}\\theta_k=1\n\\quad\\text{}\\quad\n\\sum_{k=-p}^{+f}k\\theta_k=0.\n\\]De manière générale, \\(M_\\theta\\) conserves les tendances de degré \\(d\\) si et seulement si :\n\\[\n\\sum_{k=-p}^{+f}\\theta_k=1 \n \\text{ et } \n\\forall j \\\\left\\llbracket 1,d\\right\\rrbracket:\\:\n\\sum_{k=-p}^{+f}k^j\\theta_k=0.\n\\]De manière générale, \\(M_\\theta\\) conserves les tendances de degré \\(d\\) si et seulement si :\n\\[\n\\sum_{k=-p}^{+f}\\theta_k=1 \n \\text{ et } \n\\forall j \\\\left\\llbracket 1,d\\right\\rrbracket:\\:\n\\sum_{k=-p}^{+f}k^j\\theta_k=0.\n\\]Si \\(M_\\theta\\) est symétrique (\\(p=f\\) et \\(\\theta_{-k} = \\theta_k\\)) et conserve les tendances de degré \\(2d\\) alors elle conserve aussi les tendances de degré \\(2d+1\\).Si \\(M_\\theta\\) est symétrique (\\(p=f\\) et \\(\\theta_{-k} = \\theta_k\\)) et conserve les tendances de degré \\(2d\\) alors elle conserve aussi les tendances de degré \\(2d+1\\).","code":""},{"path":"sec:propMM.html","id":"élimination-de-la-saisonnalité","chapter":"Chapitre 3 Moving average and filters","heading":"3.2.2 Élimination de la saisonnalité","text":"Soit \\(S_t\\) une série à saisonnalité fixe de périodicité \\(P\\) (\\(P=12\\) pour une série mensuelle, \\(P=4\\) pour une série trimestrielle, etc.).\ndonc \\(S_t=S_{t+P}\\) et la somme de \\(P\\) termes consécutifs est constante dans le temps7. Ainsi, une moyenne mobile simple d’ordre \\(P\\) (dont tous les coefficients sont égaux à \\(1/P\\)) supprime les saisonnalités fixes.Il est également possible de constituer des moyennes mobiles plus complexes supprimant la saisonnalité qui évoluerait polynomialement dans le temps (voir par exemple Grun-Rehomme et Ladiray (1994))","code":""},{"path":"sec:propMM.html","id":"réduction-du-bruit","chapter":"Chapitre 3 Moving average and filters","heading":"3.2.3 Réduction du bruit","text":"Toutes les séries temporelles sont affectées par du bruit qui peut brouiller l’extraction du signal.\nC’est pourquoi cherche à réduire ce bruit (en réduisant la variance) tout en conservant le signal (en utilisant les propriétés vues dans les sections précédentes).\nLa somme des carrés des coefficients \\(\\sum_{k=-p}^{+f}\\theta_k^2\\) est le rapport de réduction de la variance.En effet, soit \\(\\{\\varepsilon_t\\}\\) une suite de variables aléatoires indépendantes avec \\(\\E{\\varepsilon_t}=0\\), \\(\\V{\\varepsilon_t}=\\sigma^2\\).\n:\n\\[\n\\V{M_\\theta\\varepsilon_t}=\\V{\\sum_{k=-p}^{+f} \\theta_k \\varepsilon_{t+k}}\n= \\sum_{k=-p}^{+f} \\theta_k^2 \\V{\\varepsilon_{t+k}}=\n\\sigma^2\\sum_{k=-p}^{+f} \\theta_k^2.\n\\]","code":""},{"path":"sec:propMM.html","id":"defAsymProb","chapter":"Chapitre 3 Moving average and filters","heading":"3.3 Real-time estimation and asymmetric moving average","text":"symmetric filters, phase shift function equal zero (modulo \\(\\pi\\)).\nTherefore, delay frequency: ’s preferred asymmetric ones.\nHowever, used beginning end time series past/future value can used.\nThus, real-time estimation, needed build asymmetric moving average approximate symmetric moving average.approximation summarized quality indicators.\npaper focus ones defined Grun-Rehomme, Guggemos, et Ladiray (2018) Wildi et McElroy (2019) build asymmetric filters.Grun-Rehomme, Guggemos, et Ladiray (2018) propose general approach derive linear filters, based optimization problem three criteria: Fidelity (\\(F_g\\), noise reduction), Smoothness (\\(S_g\\)) Timeliness (\\(T_g\\), phase shift input ouput signals).\nSee section 4.2 details.Wildi et McElroy (2019) propose approach based decomposition mean squared error symmetric asymmetric filter four quantities: Accuracy (\\(A_w\\)), Timeliness (\\(T_w\\)), Smoothness (\\(S_w\\)) Residual (\\(R_w\\)).\nSee section 7 details.indicators summarized table 3.1.\nTable 3.1 : Criteria used check quality linear filter defined coefficients \\(\\theta=(\\theta_k)_{-p\\leq k\\leq f}\\) gain phase shift function, \\(\\rho_{\\theta}\\) \\(\\varphi_\\theta\\).\n","code":""},{"path":"dune-théorie-générale-sur-la-construction-des-filtres-asymétriques-à-lapproche-fst.html","id":"dune-théorie-générale-sur-la-construction-des-filtres-asymétriques-à-lapproche-fst","chapter":"Chapitre 4 D’une théorie générale sur la construction des filtres asymétriques à l’approche FST","heading":"Chapitre 4 D’une théorie générale sur la construction des filtres asymétriques à l’approche FST","text":"","code":""},{"path":"dune-théorie-générale-sur-la-construction-des-filtres-asymétriques-à-lapproche-fst.html","id":"subsec:theoriegen","chapter":"Chapitre 4 D’une théorie générale sur la construction des filtres asymétriques à l’approche FST","heading":"4.1 Théorie générale de construction des filtres asymétriques","text":"Pour tenter établir une théorie générale englobant les principaux filtres linéaires, Grun-Rehomme, Guggemos, et Ladiray (2018) définissent deux critères :\n\\[\\begin{align}\n(\\theta,q,y_t,u_t)&=\\E{(\\Delta^{q}M_\\theta y_t-u_t)^{2}} \\tag{4.1} \\\\\nJ(\\theta,f, \\omega_1,\\omega_2)&=\\int_{\\omega_1}^{\\omega_2} f\\left[\\phi_\\theta(\\omega), \\varphi_\\theta (\\omega)\\right] \\ud \\omega \\tag{4.2}\n\\end{align}\\]\noù \\(y_t\\) est la série étudiée, \\(u_t\\) une série de référence et \\(\\Delta\\) est l’opérateur différence (\\(\\Delta y_t=y_t-y_{t-1}\\) et \\(\\Delta^q=\\underbrace{\\Delta \\circ \\dots \\circ \\Delta}_{q\\text{ fois}}\\) pour \\(q\\\\N\\)).Comme nous le montrerons dans ce rapport, la majorité des filtres linéaires peut s’obtenir par une minimisation d’une somme pondérée de ces critères, sous contrainte linéaire sur les coefficients :\\[\n\\begin{cases}\n\\underset{\\theta}{\\min} & \\sum \\alpha_i (\\theta,\\, q_i,\\, y_t,\\, u_t^{()})+\n\\beta_iJ(\\theta,\\, f_i,\\, \\omega_{1,},\\, \\omega_{2,})\\\\\ns.t. & C\\theta=\n\\end{cases}\n\\]C’est en particulier le cas de l’approche Fidelity-Smoothness-Timeliness (FST) développée par les mêmes auteurs.","code":""},{"path":"dune-théorie-générale-sur-la-construction-des-filtres-asymétriques-à-lapproche-fst.html","id":"sec:GuggemosEtAl","chapter":"Chapitre 4 D’une théorie générale sur la construction des filtres asymétriques à l’approche FST","heading":"4.2 Approche Fidelity-Smoothness-Timeliness (FST)","text":"Pour construire les moyennes mobiles symétriques, Grun-Rehomme et Ladiray (1994) et Gray et Thomson (1996) proposent un programme de minimisation sous contrainte qui fait un compromis entre réduction de la variance et “lissage” de la tendance.\nGrun-Rehomme, Guggemos, et Ladiray (2018) étendent ces approches en les appliquant à la construction des filtres asymétriques et en ajoutant un critère permettant de contrôler le déphasage.\nIl s’agit de l’approche Fidelity-Smoothness-Timeliness — Fidélité-Lissage-Temporalité — (FST).Les trois critères utilisés sont les suivants :Fidelity (fidélité), \\(F_g\\) : c’est le rapport de réduction de la variance.\nPlus il est petit et plus le signal de sortie (tendance-cycle estimée) est un bon estimateur estimateur du signal à estimer (tendance-cycle).\n\\[\nF_g(\\theta) = \\sum_{k=-p}^{+f}\\theta_{k}^{2}.\n\\]\n\\(F_g\\) peut également être écrite comme une forme quadratique positive : \\(F_g(\\theta)=\\theta'F\\theta\\) avec \\(F\\) la matrice identité d’ordre \\(p+f+1\\).Fidelity (fidélité), \\(F_g\\) : c’est le rapport de réduction de la variance.\nPlus il est petit et plus le signal de sortie (tendance-cycle estimée) est un bon estimateur estimateur du signal à estimer (tendance-cycle).\n\\[\nF_g(\\theta) = \\sum_{k=-p}^{+f}\\theta_{k}^{2}.\n\\]\n\\(F_g\\) peut également être écrite comme une forme quadratique positive : \\(F_g(\\theta)=\\theta'F\\theta\\) avec \\(F\\) la matrice identité d’ordre \\(p+f+1\\).Smoothness (lissage), \\(S_g\\) :\n\\[\nS_g(\\theta) = \\sum_{j}(\\nabla^{d}\\theta_{j})^{2}.\n\\]\nCe critère mesure la proximité du signal de sortie à une tendance polynômiale de degré \\(d-1\\).\nAvec \\(d=3\\), Henderson utilise ce critère de lissage pour construire des moyennes mobiles conservant des polynômes de degré 2.\n\\(S_g\\) peut également s’écrire sous une forme quadratique positive \\(S_g(\\theta)=\\theta'S\\theta\\) avec \\(S\\) une matrice symétrique d’ordre \\(p+f+1\\) (voir section XXXX).Smoothness (lissage), \\(S_g\\) :\n\\[\nS_g(\\theta) = \\sum_{j}(\\nabla^{d}\\theta_{j})^{2}.\n\\]\nCe critère mesure la proximité du signal de sortie à une tendance polynômiale de degré \\(d-1\\).\nAvec \\(d=3\\), Henderson utilise ce critère de lissage pour construire des moyennes mobiles conservant des polynômes de degré 2.\n\\(S_g\\) peut également s’écrire sous une forme quadratique positive \\(S_g(\\theta)=\\theta'S\\theta\\) avec \\(S\\) une matrice symétrique d’ordre \\(p+f+1\\) (voir section XXXX).Timeliness (temporalité), \\(T_g\\) : il mesure le déphasage entre le signal d’entrée et le signal de sortie à des fréquences spécifiques.\nLorsqu’un filtre linéaire est appliqué, le niveau du signal d’entrée est également modifié par la fonction de gain : il est donc intuitif de considérer que plus le gain est élevé, plus l’impact du déphasage le sera.Timeliness (temporalité), \\(T_g\\) : il mesure le déphasage entre le signal d’entrée et le signal de sortie à des fréquences spécifiques.\nLorsqu’un filtre linéaire est appliqué, le niveau du signal d’entrée est également modifié par la fonction de gain : il est donc intuitif de considérer que plus le gain est élevé, plus l’impact du déphasage le sera.C’est pourquoi le critère de déphasage dépend des fonctions de gain et de déphasage (\\(\\rho_\\theta\\) et \\(\\varphi_{\\theta}\\)), le lien entre les deux fonctions étant faite à partir d’une fonction de pénalité \\(f\\)8 :\n\\[\n\\int_{\\omega_{1}}^{\\omega_{2}}f(\\rho_{\\theta}(\\omega),\\varphi_{\\theta}(\\omega))\\ud\\omega\n\\]\nComme fonction de pénalité, les auteurs suggèrent de prendre \\(f\\colon(\\rho,\\varphi)\\mapsto\\rho^2\\sin(\\varphi)^2\\).\nCela permet notamment d’avoir une timeliness qui peut s’écrire comme une forme quadratique positive (\\(T_g(\\theta)=\\theta'T\\theta\\) avec \\(T\\) une matrice carré symmétrique d’ordre \\(p+f+1\\), voir Grun-Rehomme, Guggemos, et Ladiray (2018) pour la démonstration).\nDans cet article nous utilisons \\(\\omega_1=0\\) et \\(\\omega_2=2\\pi/12\\): ne s’intéresse qu’à des séries mensuelles et au déphasage qui impactent les cycles d’au minimum 12 mois.En somme, dans l’approche FST consiste à minimiser une somme pondérée de ces trois critères sous certaines contraintes (généralement préservation polynomiales).\\[\\begin{equation}\n\\begin{cases}\n\\underset{\\theta}{\\min} &\n\\alpha F_g(\\theta)+\\beta S_g(\\theta)+\\gamma T_g(\\theta)\\\\\ns.t. & C\\theta=\n\\end{cases}. \\tag{4.3}\n\\end{equation}\\]Les conditions \\(\\alpha,\\beta,\\gamma\\geq 0\\) et \\(\\alpha\\beta\\ne 0\\) garantissent que \\(\\alpha F_g(\\theta)+\\beta S_g(\\theta)+\\gamma T_g(\\theta)\\) soit strictement convexe et donc garantissent l’unicité de la solution.\nUn autre avantage de cette approche est que les filtres asymétriques construits sont totalement indépendants des données, de la date d’estimation et du filtre symétrique choisis.obtient par exemple le filtre d’Henderson avec les paramètres suivants :\n\\[C=\\begin{pmatrix}\n1 & \\cdots&1\\\\\n-h & \\cdots&h \\\\\n(-h)^2 & \\cdots&h^2\n\\end{pmatrix},\\quad\n=\\begin{pmatrix}\n1 \\\\0\\\\0\n\\end{pmatrix},\\quad\n\\alpha=\\gamma=0,\\quad\n\\beta=1,\\quad d=3.\\]Un des inconvénients de cette approche est les différents critères ne sont pas normalisés : leurs valeurs ne peuvent pas être comparés et n’ont donc pas de sens.\nIl n’y par exemple pas d’interprétation à donner un poids deux fois plus important à la timeliness qu’à la fidelity.Les trois critères utilisés dans le programme de minimisation @(eq:gugemmos) sont des cas particuliers de ceux définis dans la section @(subsec:theoriegen).\nEn effet, en notant \\(y_t=TC_t+\\varepsilon_t,\\quad\\varepsilon_t\\sim\\Norm(0,\\sigma^2)\\) avec \\(TC_t\\) une tendance déterministe, :\n\\[\\begin{align*}\nF_g(\\theta) & = (\\theta,\\,0,\\,y_t,\\,\\E{M_\\theta y_t})\\\\\nS_g(\\theta) & = (\\theta,\\,q,\\,y_t,\\,\\E{M_\\theta y_t})\\\\\nT_g(\\theta) & = J(f\\colon(\\rho,\\varphi)\\mapsto\\rho^2\\sin(\\varphi)^2,\\,\\omega_1, \\,\\omega_2).\n\\end{align*}\\]Filtres FSTAvantages :Le filtre asymétrique est indépendant du filtre symétrique, des données et de la date d’estimation.Le filtre asymétrique est indépendant du filtre symétrique, des données et de la date d’estimation.Le problème d’optimisation admet une solution uniqueLe problème d’optimisation admet une solution uniqueInconvénients :Les différents critères ne sont pas normalisés : les poids accordés aux différents critères ne peuvent être comparés.Fonction  : rjdfilters::fst_filter().","code":""},{"path":"régression-polynomiale-locale.html","id":"régression-polynomiale-locale","chapter":"Chapitre 5 Régression polynomiale locale","heading":"Chapitre 5 Régression polynomiale locale","text":"Feng et Schäfer (2021)\nGray et Thomson (1996)\nLuati et Proietti (2011)Comme notamment montré par Loader (1999), la régression locale est un cas particulier de la régression non paramétrique.\nSupposons que l’ait un ensemble de points \\((x_i,y_i)_{1\\leq \\leq n}\\).\nLa régression non paramétrique consiste à supposer qu’il existe une fonction \\(\\mu\\), à estimer, telle que \\(y_i=\\mu(x_i)+\\varepsilon_i\\) avec \\(\\varepsilon_i\\) un terme d’erreur.\nD’après le théorème de Taylor, pour tout point \\(x_0\\), si \\(\\mu\\) est différentiable \\(p\\) alors :\n\\[\n\\forall x \\::\\:\\mu(x) = \\mu(x_0) + \\mu'(x_0)(x-x_0)+\\dots +\n\\frac{\\mu^{(p)}(x_0)}{p!}(x-)^p+R_p(x),\n\\]\noù \\(R_p\\) est un terme résiduel négligeable au voisinage de \\(x_0\\).\nDans un voisinage \\(\\left[x_0-h(x_0),x_0-h(x_0)\\right]\\) de \\(x_0\\), \\(\\mu\\) peut être approchée par un polynôme de degré \\(d\\).\nLa quantité \\(h(x_0)\\) est appelée fenêtre (bandwidth).\nSi \\(\\varepsilon_i\\) est un bruit blanc, peut donc estimer par moindre carrés \\(\\mu(x_0)\\) en utilisant les observations qui sont dans \\(\\left[x_0-h(x_0),x_0-h(x_0)\\right]\\).","code":""},{"path":"régression-polynomiale-locale.html","id":"approche-de-proietti2008","chapter":"Chapitre 5 Régression polynomiale locale","heading":"5.1 Approche de Proietti et Luati (2008)","text":"Reprenons maintenant les notations de Proietti et Luati (2008): supposons que notre série temporelle \\(y_t\\) peut être décomposée en\n\\[\ny_t=\\mu_t+\\varepsilon_t,\n\\]\noù \\(\\mu_t\\) est la tendance et \\(\\varepsilon_{t}\\overset{..d}{\\sim}\\mathcal{N}(0,\\sigma^{2})\\) est le bruit9.\nLa tendance \\(\\mu_t\\) est localement approché par un polynôme de degré \\(d\\), de sorte que dans un voisinage \\(h\\) de \\(t\\) \\(\\mu_t\\simeq m_{t}\\) avec :\n\\[\n\\forall j\\\\left\\llbracket -h,h\\right\\rrbracket :\\:\ny_{t+j}=m_{t+j}+\\varepsilon_{t+j},\\quad m_{t+j}=\\sum_{=0}^{d}\\beta_{}j^{}\n\\]\nLe problème d’extraction de la tendance est équivalent à l’estimation de \\(m_t=\\beta_0\\).\nEn notation matricielle :\n\\[\n\\underbrace{\\begin{pmatrix}y_{t-h}\\\\\ny_{t-(h-1)}\\\\\n\\vdots\\\\\ny_{t}\\\\\n\\vdots\\\\\ny_{t+(h-1)}\\\\\ny_{t+h}\n\\end{pmatrix}}_{y}=\\underbrace{\\begin{pmatrix}1 & -h & h^{2} & \\cdots & (-h)^{d}\\\\\n1 & -(h-1) & (h-1)^{2} & \\cdots & (-(h-1))^{d}\\\\\n\\vdots & \\vdots & \\vdots & \\cdots & \\vdots\\\\\n1 & 0 & 0 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\cdots & \\vdots\\\\\n1 & h-1 & (h-1)^{2} & \\cdots & (h-1)^{d}\\\\\n1 & h & h^{2} & \\cdots & h^{d}\n\\end{pmatrix}}_{X}\\underbrace{\\begin{pmatrix}\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\vdots\\\\\n\\vdots\\\\\n\\vdots\\\\\n\\beta_{d}\n\\end{pmatrix}}_{\\beta}+\\underbrace{\\begin{pmatrix}\\varepsilon_{t-h}\\\\\n\\varepsilon_{t-(h-1)}\\\\\n\\vdots\\\\\n\\varepsilon_{t}\\\\\n\\vdots\\\\\n\\varepsilon_{t+(h-1)}\\\\\n\\varepsilon_{t+h}\n\\end{pmatrix}}_{\\varepsilon}\n\\]\nPour estimer \\(\\beta\\) il faut \\(H\\geq d+1\\) et l’estimation est faite par moindres carrés pondérés — weighted least squares (WLS) —, ce qui à minimiser la fonction objectif suivante :\n\\[\nS(\\hat{\\beta}_{0},\\dots,\\hat{\\beta}_{d})=\\sum_{j=-h}^{h}\\kappa_{j}(y_{t+j}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}j-\\dots-\\hat{\\beta}_{d}j^{d})^{2}\n\\]\noù \\(\\kappa_j\\) est un ensemble de poids appelés noyaux (kernel).\n\\(\\kappa_j\\geq 0:\\kappa_{-j}=\\kappa_j\\), et en notant \\(K=diag(\\kappa_{-h},\\dots,\\kappa_{h})\\), l’estimateur \\(\\beta\\) peut s’écrire \\(\\hat{\\beta}=(X'KX)^{1}X'Ky\\).\nAvec \\(e_{1}=\\begin{pmatrix}1&0&\\cdots&0\\end{pmatrix}'\\), l’estimateur de la tendance peut donc s’écrire :\n\\[\n\\hat{m}_{t}=e_{1}\\hat{\\beta}=w'y=\\sum_{j=-h}^{h}w_{j}y_{t-j}\\text{ }w=KX(X'KX)^{-1}e_{1}\n\\]\nEn somme, l’estimation de la tendance \\(\\hat{m}_{t}\\) est obtenue en appliquant une moyenne mobile symétrique \\(w\\) à \\(y_t\\)10.\nDe plus, \\(X'w=e_{1}\\) donc :\n\\[\n\\sum_{j=-h}^{h}w_{j}=1,\\quad\\forall r\\\\left\\llbracket 1,d\\right\\rrbracket :\\sum_{j=-h}^{h}j^{r}w_{j}=0.\n\\]\nAinsi, la moyenne mobile \\(w\\) préserve les polynômes de degré \\(d\\).","code":""},{"path":"régression-polynomiale-locale.html","id":"sec:kernels","chapter":"Chapitre 5 Régression polynomiale locale","heading":"5.1.1 Les différents noyaux","text":"Dans les problèmes d’extraction du signal, les observations sont généralement pondérées par rapport à leur distance à la date \\(t\\) : pour estimer la tendance-cycle à la date \\(t\\), les observations celles qui sont proches de \\(t\\).Dans le cas continu, un noyau \\(K\\) est une fonction positive, paire et intégrable telle que \\(\\int_{-\\infty}^{+\\infty}\\kappa(u) \\ud u=1\\) et \\(\\kappa(u)=\\kappa(-u)\\).Dans le cas discret, un noyau est un ensemble de poids \\(\\kappa_j\\), \\(j=0,\\pm1,\\dots,\\pm h\\) avec \\(\\kappa_j \\geq0\\) et \\(\\kappa_j=\\kappa_{-j}\\).Une classe importante de noyaux est les noyaux Beta.\nDans le cas discret, à un factor multiplicatif près (de sorte que \\(\\sum_{j=-h}^h\\kappa_j=1\\)) :\n\\[\n\\kappa_j = \\left(\n  1-\n  \\left\\lvert\n  \\frac j {h+1}\n  \\right\\lvert^r\n\\right)^s,\\quad\\text{avec }r>0,s\\geq 0\n\\]\nCette classe engloble la majorité des noyaux présentés dans ce rapport, à l’exception du noyau d’Henderson, trapézoïdal et gaussien.\nLes principaux noyaux (qui sont également implémentés dans rjdfilters) sont :\\(r=1,s=0\\) noyau uniforme :\n\\[\\kappa_j^U=1\\]\\(r=1,s=0\\) noyau uniforme :\n\\[\\kappa_j^U=1\\]\\(r=s=1\\) noyau triangulaire :\n\\[\\kappa_j^T=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert\n\\right)\\]\\(r=s=1\\) noyau triangulaire :\n\\[\\kappa_j^T=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert\n\\right)\\]\\(r=2,s=1\\) noyau d’Epanechnikov (ou parabolique) :\n\\[\\kappa_j^E=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert^2\n\\right)\\]\\(r=2,s=1\\) noyau d’Epanechnikov (ou parabolique) :\n\\[\\kappa_j^E=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert^2\n\\right)\\]\\(r=s=2\\) noyau quadratique (biweight) :\n\\[\\kappa_j^{BW}=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert^2\n\\right)^2\\]\\(r=s=2\\) noyau quadratique (biweight) :\n\\[\\kappa_j^{BW}=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert^2\n\\right)^2\\]\\(r = 2, s = 3\\) noyau cubique (triweigh) :\n\\[\\kappa_j^{TW}=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert^2\n\\right)^3\\]\\(r = 2, s = 3\\) noyau cubique (triweigh) :\n\\[\\kappa_j^{TW}=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert^2\n\\right)^3\\]\\(r = s = 3\\) noyau tricube :\n\\[\\kappa_j^{TC}=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert^3\n\\right)^3\\]\\(r = s = 3\\) noyau tricube :\n\\[\\kappa_j^{TC}=\\left(\n1-\n\\left\\lvert\n\\frac j {h+1}\n\\right\\lvert^3\n\\right)^3\\]noyau d’Henderson (see section 5.1.2 details):\n\\[\n\\kappa_{j}=\\left[1-\\frac{j^2}{(h+1)^2}\\right]\n\\left[1-\\frac{j^2}{(h+2)^2}\\right]\n\\left[1-\\frac{j^2}{(h+3)^2}\\right]\n\\]noyau d’Henderson (see section 5.1.2 details):\n\\[\n\\kappa_{j}=\\left[1-\\frac{j^2}{(h+1)^2}\\right]\n\\left[1-\\frac{j^2}{(h+2)^2}\\right]\n\\left[1-\\frac{j^2}{(h+3)^2}\\right]\n\\]noyau trapézoidal :\n\\[\n\\kappa_j^{TP}=\n\\begin{cases}\n\\frac{1}{3(2h-1)} & \\text{ }j=\\pm h \n\\\\\n\\frac{2}{3(2h-1)} & \\text{ }j=\\pm (h-1)\\\\\n\\frac{1}{2h-1}& \\text{ otherwise}\n\\end{cases}\n\\]noyau trapézoidal :\n\\[\n\\kappa_j^{TP}=\n\\begin{cases}\n\\frac{1}{3(2h-1)} & \\text{ }j=\\pm h \n\\\\\n\\frac{2}{3(2h-1)} & \\text{ }j=\\pm (h-1)\\\\\n\\frac{1}{2h-1}& \\text{ otherwise}\n\\end{cases}\n\\]noyau gaussien11:\n\\[\n\\kappa_j^G=\\exp\\left(\n-\\frac{\nj^2\n}{\n2\\sigma^2h^2\n}\\right)\n\\]noyau gaussien11:\n\\[\n\\kappa_j^G=\\exp\\left(\n-\\frac{\nj^2\n}{\n2\\sigma^2h^2\n}\\right)\n\\]Les noyaux d’Henderson, trapézoïdal gaussien sont particuliers :Les fonctions noyau d’Henderson et trapézoïdal changent avec la fenêtre (les autres dépendent uniquement du rapport \\(j/h+1\\)).Les fonctions noyau d’Henderson et trapézoïdal changent avec la fenêtre (les autres dépendent uniquement du rapport \\(j/h+1\\)).Pour les noyaux trapézoïdal et gaussien, d’autres définitions pourraient être utilisées et sont donc définis arbitrairement.Pour les noyaux trapézoïdal et gaussien, d’autres définitions pourraient être utilisées et sont donc définis arbitrairement.Le noyau trapézoïdal est implémenté dans rjdfilters car il permet d’extraire les moyennes mobiles utilisées dans l’algorithme X-12ARIMA pour l’extraction des composantes saisonnières.\nIl n’est pas adapté dans le cas de l’extraction de la tendance-cycle.figure ?? summarizes coefficients different kernels.\nAnalyzing coefficients can already anticipate properties associated filters:triweight kernel narrowest distribution.\nnarrowest distribution , smallest weights furthest neighbors : associated filter high weight current observation (\\(t\\)).triweight kernel narrowest distribution.\nnarrowest distribution , smallest weights furthest neighbors : associated filter high weight current observation (\\(t\\)).\\(h\\) high Henderson kernel equivalent triweight kernel (since \\(h+1\\sim h+2 \\sim h+3\\), \\(\\kappa_j^H\\sim\\kappa_j^{TW}\\)), associated filter also equivalent.\nHowever, \\(h\\) small (\\(h\\leq10\\)) Henderson kernel closer biweight kernel triweight kernel.\\(h\\) high Henderson kernel equivalent triweight kernel (since \\(h+1\\sim h+2 \\sim h+3\\), \\(\\kappa_j^H\\sim\\kappa_j^{TW}\\)), associated filter also equivalent.\nHowever, \\(h\\) small (\\(h\\leq10\\)) Henderson kernel closer biweight kernel triweight kernel.","code":""},{"path":"régression-polynomiale-locale.html","id":"sec:sympolyfilter","chapter":"Chapitre 5 Régression polynomiale locale","heading":"5.1.2 Specific symmetric filters","text":"\\(p=0\\) (local adjustment constant) obtain Nadaraya-Watson’s estimator.uniform kernel obtain Macaulay filter.\n\\(p=0,1\\), arithmetic moving average: \\(w_j=w=\\frac{1}{2h+1}\\).Epanechnikov kernel often recommended optimal kernel minimizes mean square error estimation local polynomial.Loess locally weighted polynomial regression uses tricube kernel.Henderson filter specific case local cubic fit (\\(p=3\\)), widely used trend estimation (example ’s filter used seasonal adjustment software X-12ARIMA). fixed bandwidth, Henderson found kernel gave smoothest estimates trend.\nshowed three following problems equivalent:minimize variance third difference series application moving average;minimize sum squares third difference coefficients filter, ’s smoothness criterion: \\(S=\\sum_j(\\nabla^{3}\\theta_{j})^{2}\\);fit local cubic polynomial weighted least squares, weights chose minimize sum squares resulting filter.Resolving last problem leads kernel presented section 5.1.1.","code":""},{"path":"régression-polynomiale-locale.html","id":"analysis-of-symmetric-filters","chapter":"Chapitre 5 Régression polynomiale locale","heading":"5.1.3 Analysis of symmetric filters","text":"section, filters computed local polynomial degree \\(d=3\\).\nfigure ?? plots coefficients filters different kernels presented different kernels presented section 5.1.1 different bandwidths \\(h\\).\ntable 5.1 shows variance reduction different filters.\nfind similar results section ??:triweight kernel gives filter narrowest distribution.\nnarrowest distribution , higher variance reduction .\nIndeed, distribution coefficients filter can interpreted output signal additive outlier.\nresult, wide distribution, additive outlier persistent narrow distribution.\nTherefore, ’s triweight higher variance reduction \\(h\\leq30\\).triweight kernel gives filter narrowest distribution.\nnarrowest distribution , higher variance reduction .\nIndeed, distribution coefficients filter can interpreted output signal additive outlier.\nresult, wide distribution, additive outlier persistent narrow distribution.\nTherefore, ’s triweight higher variance reduction \\(h\\leq30\\).\\(h\\) small, trapezoidal filter seems produce similar results Epanechnikov one.\\(h\\) small, trapezoidal filter seems produce similar results Epanechnikov one.\\(h\\) small Henderson filter closed biweight kernel, \\(h\\) high equivalent triweight kernel.\\(h\\) small Henderson filter closed biweight kernel, \\(h\\) high equivalent triweight kernel.\nTable 5.1 : Variance reduction ratio (\\(\\sum\\theta_i^2\\)) symmetric filters computed local polynomial degree \\(3\\).\nMoreover, find filters, coefficients decrease, distance central observation increases, negative value increase towards 0 (except uniform kernel).\nNegative coefficients might disturbing arise cubic polynomial constraints.\nIndeed preserve polynomial degree 2 (3) need \\(\\sum_{j=-h}^hj^2\\theta_i=0\\), constraint coefficients negative.\nHowever, negative coefficients negligible compare central coefficients (80% smaller central coefficient kernels, except uniform trapezoidal high bandwidth).","code":""},{"path":"régression-polynomiale-locale.html","id":"gain-functions","chapter":"Chapitre 5 Régression polynomiale locale","heading":"5.1.4 Gain functions","text":"Figure ?? plots gain functions different filters.\nGain functions usually plotted 0 \\(\\pi\\).\nHowever, locally weighted polynomial regression low-pass filters: leave almost unchanged low frequency components (trend) attenuate high frequency fluctuations (noise).\nmonthly data, cycle 3 years corresponds frequency \\(2\\pi/36\\) cycle 7 years frequency \\(2\\pi/84\\).\nTherefore, ideal pass-band filter gain function equal 1 low frequency (\\(\\leq 2\\pi/36\\)) equal 0 frequencies.bandwidth \\(h\\) increases, gain function decreases low frequencies: short business cycles attenuated.\nfixed value \\(h\\), gaussian, Henderson triweight filters preserve short business cycles filters (especially uniform, trapezoidal Epanechnikov).\nMoreover, gain function filters decreases faster zero less fluctuations: enhances higher variance reduction ratio shown table 5.1.\nNote: two horizontal lines corresponds frequencies \\(2\\pi/84\\) (cycle 7 years) \\(2\\pi/36\\) (cycle 3 years).\nJust analyzing symmetric filters properties, doubt Henderson, triweight biweight filters similar properties perform better kernel trend-cycle extraction.\nresults found asymmetric filters.\nThus, order simplify presentation analysis, next sections show results Henderson filter.","code":""},{"path":"régression-polynomiale-locale.html","id":"asymmetric-filters","chapter":"Chapitre 5 Régression polynomiale locale","heading":"5.2 Asymmetric filters","text":"","code":""},{"path":"régression-polynomiale-locale.html","id":"direct-asymmetric-filters-daf","chapter":"Chapitre 5 Régression polynomiale locale","heading":"5.2.1 Direct asymmetric filters (DAF)","text":"mentioned section 3.3, symmetric filters used boundary points. real-time estimation, three different approaches can used:Build asymmetric filter fitting local polynomial available observations \\(y_{t}\\) \\(t\\\\left\\llbracket n-h,n\\right\\rrbracket\\).Build asymmetric filter fitting local polynomial available observations \\(y_{t}\\) \\(t\\\\left\\llbracket n-h,n\\right\\rrbracket\\).Apply symmetric filter series extended forecast (backcast) \\(\\hat{y}_{n+l\\mid n},l\\\\left\\llbracket 1,h\\right\\rrbracket\\).Apply symmetric filter series extended forecast (backcast) \\(\\hat{y}_{n+l\\mid n},l\\\\left\\llbracket 1,h\\right\\rrbracket\\).Build asymmetric filter minimize mean square revision error subject polynomial reproducing constraints.Build asymmetric filter minimize mean square revision error subject polynomial reproducing constraints.Proietti et Luati (2008) show first two approaches equivalent forecast done polynomial extrapolation order \\(d\\) (forecasts generated polynomial model symmetric filter).\ncalled direct asymmetric filter (DAF).\nLet \\(q\\) number available observations future: \\(q\\) varies 0 (real time filter) \\(h\\) (symmetric filter).Rewriting matrix \\(X\\), \\(K\\) \\(y\\) following way:\n\\[\nX=\\begin{pmatrix}X_{p}\\\\\nX_{f}\n\\end{pmatrix},\\quad y=\\begin{pmatrix}y_{p}\\\\\ny_{f}\n\\end{pmatrix},\\quad K=\\begin{pmatrix}K_{p} & 0\\\\\n0 & K_{f}\n\\end{pmatrix}\n\\]\n\\(y_{p}\\) correspond available data \\(y_{f}\\) missing data.\nDAF \\(w_a\\) forecast \\(\\hat{y}_{f}\\) can written :\n\\[\nw_{}=K_{p}X_{p}(X'_{p}K_{p}X_{p})^{-1}e_{1},\n\\quad\n\\hat{y}_{f}=X_{f}(X'_{p}K_{p}X_{p})^{-1}X_{p}'K_{p}y_{p}\n\\]\nMoreover, following results DAF \\(w_a\\):satisfies polynomial reproduction constraints symmetric filter (conserve polynomial degree \\(d\\)).\nThus, bias estimating unknown function time order magnitude interior time support.satisfies polynomial reproduction constraints symmetric filter (conserve polynomial degree \\(d\\)).\nThus, bias estimating unknown function time order magnitude interior time support.\\(w_a\\) minimize weighted distance (kernel function) asymmetric filter coefficients symmetric ones.\nTherefore, DAF equivalent fit local polynomial minimize revisions\\(w_a\\) minimize weighted distance (kernel function) asymmetric filter coefficients symmetric ones.\nTherefore, DAF equivalent fit local polynomial minimize revisionsHowever, weights \\(w_{,0}\\) DAF highly concentrated current observation \\(t\\) important change \\(q=0\\) (real-time filter) \\(q=h\\) (see figure 5.1).\nMoreover real-time filter doesn’t satisfying gain functions: closer one frequencies (thus low noise reduction power).\nTherefore, even real-time filter unbiased (series generated polynomial degree \\(d\\)) expenses high variance.\nFigure 5.1 : Coefficients gain function direct asymmetric filters (DAF) computed local polynomial degree \\(3\\) Henderson kernel \\(h=6\\).\nkernels, find results Proietti et Luati (2008):fixed value \\(d\\), data available (\\(q\\) increases), weight associated current observation \\(w_{,0}\\) decreases.fixed value \\(d\\), data available (\\(q\\) increases), weight associated current observation \\(w_{,0}\\) decreases.fixed value \\(h\\) \\(q\\), \\(w_{,0}\\) increases exponentially polynomial degree \\(d\\) (particular, \\(d=h\\), \\(w_{,0}=1\\)).fixed value \\(h\\) \\(q\\), \\(w_{,0}\\) increases exponentially polynomial degree \\(d\\) (particular, \\(d=h\\), \\(w_{,0}=1\\)).","code":""},{"path":"régression-polynomiale-locale.html","id":"subsec:lppasymf","chapter":"Chapitre 5 Régression polynomiale locale","heading":"5.2.2 General class of asymmetric filters","text":"deal problem variance estimates real-time filters, Proietti et Luati (2008) suggest general asymmetric filters make tradeoff bias variance.\ngeneralisation Musgrave asymmetric filters (used seasonal adjustment algorithm X-12ARIMA, see Musgrave (1964)).consider data generated model:\n\\[\ny=U\\gamma+Z\\delta+\\varepsilon,\\quad\n\\varepsilon\\sim\\mathcal{N}(0,D)\n\\]\ngoal find filter \\(v\\) minimize mean square revision error (symmetric filter \\(w\\)) subject constraints.\nconstraints summarized matrix \\(U=\\begin{pmatrix}U_{p}'&U_{f}'\\end{pmatrix}'\\) (\\(U_p\\) available observations matrix \\(U\\) asymmetric filter): \\(U_p'v=U'w\\).\nproblem equivalent find \\(v\\) minimize:\n\\[\\begin{equation}\n\\varphi(v)=\n\\underbrace{\n  \\underbrace{(v-w_{p})'D_{p}(v-w_{p})+\n  w_{f}'D_{f}w_{f}}_\\text{revision error variance}+\n  \\underbrace{[\\delta'(Z_{p}'v-Z'w)]^{2}}_{biais^2}\n}_\\text{Mean square revision error}+\n\\underbrace{2l'(U_{p}'v-U'w)}_{\\text{constraints}}\n\\tag{5.1}\n\\end{equation}\\]\n\\(l\\) vector Lagrange multipliers.\\(U=X\\) equivalent constraint preserve polynomial degree \\(d\\): find direct asymmetric filters \\(w_a\\) \\(D=K^{-1}\\).\\(U=\\begin{pmatrix}1&\\cdots&1\\end{pmatrix}'\\), \\(Z=\\begin{pmatrix}-h&\\cdots&+h\\end{pmatrix}'\\), \\(\\delta=\\delta_1\\), \\(D=\\sigma^2I\\) symmetric filter Henderson filter obtain Musgrave asymmetric filters.\nfilter assume data generated linear process asymmetric filters preserve constant signals (\\(\\sum v_i=\\sum w_i=1\\)).\nasymmetric filters depends ratio \\(\\delta_1/\\sigma\\), related “-C” ratio \\(R=\\frac{\\bar{}}{\\bar{C}}=\\frac{\\sum\\lvert I_t-I_{t-1}\\rvert}{\\sum\\lvert C_t-C_{t-1}\\rvert}\\) (\\(\\delta_1/\\sigma=2/(R\\sqrt{\\pi})\\)), ratio expected absolute difference irregular trend-cycle.\nseasonal adjustment method, -C ratio12 used determine bandwidth use Henderson filter. monthly data:\\(R<1\\) 9-term Henderson used (\\(h=4\\));\\(R<1\\) 9-term Henderson used (\\(h=4\\));\\(1\\leq R\\leq3.5\\) 13-term Henderson used (\\(h=6\\));\\(1\\leq R\\leq3.5\\) 13-term Henderson used (\\(h=6\\));\\(3.5< R\\) 23-term Henderson used (\\(h=12\\)).\\(3.5< R\\) 23-term Henderson used (\\(h=12\\)).report, simplicity consider 13-term symmetric filters: ratio \\(\\delta^2/\\sigma^2\\) fixed \\(3.5\\).\\(U\\) corresponds first \\(d^*+1\\) first columns \\(X\\), \\(d^*<d\\), constraint asymmetric filter reproduce polynomial degree \\(d^*\\), potential bias depends value \\(\\delta\\).\nreduce variance expense bias: idea followed Proietti et Luati (2008) propose three class asymmetric filters:Linear-Constant (LC): \\(y_t\\) linear (\\(d=1\\)) \\(v\\) preserves constant signals (\\(d^*=0\\)).\nobtain Musgrave filters Henderson kernel used.Linear-Constant (LC): \\(y_t\\) linear (\\(d=1\\)) \\(v\\) preserves constant signals (\\(d^*=0\\)).\nobtain Musgrave filters Henderson kernel used.Quadratic-Linear (QL): \\(y_t\\) quadratic (\\(d=2\\)) \\(v\\) preserves linear signals (\\(d^*=1\\)).Quadratic-Linear (QL): \\(y_t\\) quadratic (\\(d=2\\)) \\(v\\) preserves linear signals (\\(d^*=1\\)).Cubic-Quadratic (CQ): \\(y_t\\) cubic (\\(d=3\\)) \\(v\\) preserves quadratic signals (\\(d^*=2\\)).Cubic-Quadratic (CQ): \\(y_t\\) cubic (\\(d=3\\)) \\(v\\) preserves quadratic signals (\\(d^*=2\\)).table 5.2 shows quality criteria different methods Henderson kernel \\(h=6\\).\nreal-time filters (\\(q=0\\)), complex filter (terms polynomial preservation), less timeliness fidelity/smoothness : reduction time-delay expense increased variance.\nchange \\(q\\) increases: \\(q=2\\) QL filter greater timeliness LC filter.\nunexpected result underlines fact approach Proietti et Luati (2008), timeliness never set goal minimize.\nTable 5.2 : Quality criteria asymmetric filters (\\(q=0,1,2\\)) computed local polynomial Henderson kernel \\(h=6\\) \\(R=3.5\\).\nRegarding mean square revision error (\\(A_w+S_w+T_w+R_w\\)), LC QL filters always gives better results CQ DAF filters.\n“theorical” mean square revision error can compared “empirical” one computed applying filters real data.\ntable 5.3 shows average mean square revision error different filters applied Industrial production indices European Union 2003 2019.\nmean square revision errors different level give results comparing different filters: validate formula used criteria \\(A_w,S_w,T_w,\\text{ }R_w\\) (see section 7).Taking value \\(R\\) “-C” ratio computed raw data gives summary results.\nTable 5.3 : Mean squared revision error asymmetric filters (\\(q=0,1,2\\)) computed local polynomial Industrial production indices European Union.\n results suggest focusing LC QL filters focus asymmetric linear filters preserve polynomial trends degree equal less one.results different kernels can also visualized online application available https://aqlt.shinyapps.io/FiltersProperties/.Local polynomial filtersAdvantages:Simple models easy interpretation.Simple models easy interpretation.asymmetric linear filter independent date estimation.\nHowever, depends data calibrate filter “-C” ratio.asymmetric linear filter independent date estimation.\nHowever, depends data calibrate filter “-C” ratio.Drawbacks:Timeliness controlled.","code":""},{"path":"régression-polynomiale-locale.html","id":"choix-des-différents-paramètres","chapter":"Chapitre 5 Régression polynomiale locale","heading":"5.3 Choix des différents paramètres","text":"Deux paramètres sont cruciaux dans la précision de l’approximation :le degré du polynôme \\(d\\) : s’il est trop petit risque d’avoir des estimations biaisés de la tendance-cycle et s’il est trop grand le alors risque d’avoir une trop grande variance dans les estimations (du fait d’un sur-ajustement) ;le degré du polynôme \\(d\\) : s’il est trop petit risque d’avoir des estimations biaisés de la tendance-cycle et s’il est trop grand le alors risque d’avoir une trop grande variance dans les estimations (du fait d’un sur-ajustement) ;le nombre de voisins \\(H=2h+1\\) (ou la fenêtre \\(h\\)) : s’il est trop petit alors trop peu de données seront utilisées pour les estimations (ce qui conduira à une grande variance dans les estimations) et s’il est trop grand alors l’approximation polynomiale sera vraisemblablement fausse ce qui conduira à avoir des estimations biaisées.le nombre de voisins \\(H=2h+1\\) (ou la fenêtre \\(h\\)) : s’il est trop petit alors trop peu de données seront utilisées pour les estimations (ce qui conduira à une grande variance dans les estimations) et s’il est trop grand alors l’approximation polynomiale sera vraisemblablement fausse ce qui conduira à avoir des estimations biaisées.","code":""},{"path":"sec:Dagum.html","id":"sec:Dagum","chapter":"Chapitre 6 Filtres et Reproducing Kernel Hilbert Space (RKHS)","heading":"Chapitre 6 Filtres et Reproducing Kernel Hilbert Space (RKHS)","text":"La théorie des Reproducing Kernel Hilber Space (RKHS) — espaces de Hilbert à noyau reproduisant — est une théorie générale dans l’apprentissage statistique non-paramétrique qui permet d’englober un grand nombre de méthodes.\nC’est par exemple le cas des méthodes de régression par moindres carrés pénalisés, des Support Vector Machine (SVM), du filtre d’Hodrick-Prescott (utilisé )Classical non-parametric filters (Henderson, LOESS, Hodrick-Prescott) can characterized using Reproducing Kernel Hilbert Space (RKHS) methodology, described Dagum et Bianconcini (2008).\nallows develop derive linear filters associated asymmetric filters. ??RKHS Hilbert space characterized kernel reproduces, via inner product defined density function \\(f_0(t)\\), every function space.\nTherefore, kernel estimator \\(K_p\\) order \\(p\\) (.e.: reproduce without distortion polynomial trend degree \\(p-1\\)) can decomposed product reproducing kernel \\(R_{p-1}\\), belonging space polynomials degree \\(p-1\\), probability density function \\(f_0\\).sequence \\(\\left(P_{}\\right)_{0\\leq \\leq p-1}\\) orthonormal polynomials \\(\\mathbb{L}^{2}(f_{0})\\)13, kernel estimator \\(K_p\\) defined :\n\\[\nK_{p}(t)=\\sum_{=0}^{p-1}P_{}(t)P_{}(0)f_{0}(t)\n\\]\nweights symmetric filter derived :\n\\[\n\\forall j\\\\left\\llbracket -h,h\\right\\rrbracket\\::\\: w_{j}=\\frac{K_p(j/b)}{\\sum_{=-h}^{^h}K_p(/b)}\n\\]\n\\(b\\) time-invariant global bandwidth parameter.density \\(f_0\\) corresponds continuous versions kernel defined 5.1.1.\nexample, biweight function \\(f_{0B}(t)=(15/16)(1-t^2)^2,t\\[-1,1]\\).\nlocal polynomial filter obtained biweight kernel obtained using bandwidth \\(b=h+1\\).goal Dagum et Bianconcini (2008) derive asymmetric filters Henderson symmetric filter, therefore ideal kernel function Henderson one.\nHowever, shown section 5.1.1, Henderson density function bandwidth needs calculated time \\(m\\) changes (corresponding orthonormal polynomial).\n’s authors use biweight kernel approximate Henderson kernel (\\(h\\geq 24\\) suggest consider triweight kernel).asymmetric weighted obtained adapting kernels length asymmetric filters:\n\\[\n\\forall j\\\\left\\llbracket -h,q\\right\\rrbracket\\::\\: w_{,j}=\\frac{K_p(j/b)}{\\sum_{=-h}^{^q}K_p(/b)}\n\\]\n\\(b=h+1\\), Proietti et Luati (2008) show obtain direct asymmetric filters (DAF).Bee Dagum et Bianconcini (2015), authors suggest performing optimal bandwidth selection (parameter \\(b\\)), decomposing mean squared revision error equation (7.2) uniform spectral density (\\(h(\\omega)=1\\)).\nbandwidth can chosen minimize mean squared revision error, phase shift, etc.\nfollowing bandwidth selection studied:\n\\[\\begin{align*}\nb_{q,G}&=\\underset{b_q\\]h;2 h+1]}{\\min}\n\\sqrt{2\\int_{0}^{\\pi}\n\\left(\\rho_s(\\omega)-\\rho_\\theta(\\omega)\\right)^{2}\\ud \\omega\n}\\\\\nb_{q,\\gamma}&=\\underset{b_q\\]h;2 h+1]}{\\min}\n\\sqrt{2\\int_{0}^{\\pi}\n\\lvert \\Gamma_s(\\omega)-\\Gamma_\\theta(\\omega)\\rvert^2\\ud \\omega\n} \\\\\nb_{q,\\varphi}&=\\underset{b_q\\]h;2 h+1]}{\\min}\n8\\int_{0}^{2\\pi/12}\n\\rho_s(\\lambda)\\rho_\\theta(\\lambda)\\sin^{2}\\left(\\frac{\\varphi_\\theta(\\omega)}{2}\\right)\\ud \\omega\n\\end{align*}\\]One drawbacks optimal bandwidth selection guarantee unique solution.coherent definitions sections, formulas \\(b_{q,G}\\), \\(b_{q,\\gamma}\\) \\(b_{q,\\varphi}\\) slightly differ ones defined Bee Dagum et Bianconcini (2015) :\\(b_{q,\\varphi}\\) defined \n\\[\nb_{q,\\varphi}=\\underset{b_q\\]h;2 h+1]}{\\min}\n\\sqrt{2\\int_{\\Omega_S}\n\\rho_s(\\lambda)\\rho_\\theta(\\lambda)\\sin^{2}\\left(\\frac{\\varphi_\\theta(\\omega)}{2}\\right)\\ud \\omega}\n\\]\\(\\Omega_S=[0,2\\pi/36]\\) frequency domain associated cycles 16 months longer.different formula used frequency response function (\\(\\Gamma_\\theta(\\omega)=\\sum_{k=-p}^{+f} \\theta_k e^{2\\pi \\omega k}\\)): changes interval integration.table 6.1 shows quality criteria RKHS filters.\nEven symmetric filter preserves quadratic trends, case asymmetric filters: preserve constant trends.\ndata available (.e.: \\(q\\) increases), less asymmetric filters distort polynomial trends (example, \\(q=2\\), \\(b_l\\simeq0\\) \\(b_{q,G}\\)).\nTable 6.1 : Quality criteria asymmetric filters (\\(q=0,1,2\\)) computed RKHS methodology \\(h=6\\).\nRKHS filtersAdvantages:asymmetric linear filter independent data date estimation.asymmetric linear filter independent data date estimation.Filters apply irregular frequency series (example lot missing values) can easily computed.Filters apply irregular frequency series (example lot missing values) can easily computed.Drawbacks:linear filters don’t preserve polynomial trends degree 1 .linear filters don’t preserve polynomial trends degree 1 .optimization problems might occur (several minimum, etc.).optimization problems might occur (several minimum, etc.).\nFigure 6.1 : toto\n","code":""},{"path":"sec:WildiMcLeroy.html","id":"sec:WildiMcLeroy","chapter":"Chapitre 7 Data-dependent filter","heading":"Chapitre 7 Data-dependent filter","text":"Wildi et McElroy (2019), authors proposed data-dependent approach derive linear filters. decompose mean square revision error trilemma three quantities: accuracy, timeliness smoothness.\nContrary Ladiray (2018), decomposition implies values three criteria comparable.Let:\\(\\left\\{ x_{t}\\right\\}\\) input time series;\\(\\left\\{ x_{t}\\right\\}\\) input time series;\\(\\left\\{y_{t}\\right\\}\\) target signal, .e. result symmetric filter, \\(\\Gamma_s\\), \\(\\rho_s\\) \\(\\varphi_s\\) associated frequency response, gain phase shift functions.\\(\\left\\{y_{t}\\right\\}\\) target signal, .e. result symmetric filter, \\(\\Gamma_s\\), \\(\\rho_s\\) \\(\\varphi_s\\) associated frequency response, gain phase shift functions.\\(\\left\\{\\hat y_{t}\\right\\}\\) estimation \\(\\left\\{y_{t}\\right\\}\\), .e. result asymmetric filter (observations available), \\(\\Gamma_\\theta\\), \\(\\rho_\\theta\\) \\(\\varphi_\\theta\\) associated frequency response, gain phase shift functions.\\(\\left\\{\\hat y_{t}\\right\\}\\) estimation \\(\\left\\{y_{t}\\right\\}\\), .e. result asymmetric filter (observations available), \\(\\Gamma_\\theta\\), \\(\\rho_\\theta\\) \\(\\varphi_\\theta\\) associated frequency response, gain phase shift functions.assume \\(\\left\\{ x_{t}\\right\\}\\) weakly stationary continuous spectral density \\(h\\), mean square revision error, \\(\\E{(y_{t}-\\hat{y}_{t})^{2}}\\), can written :\n\\[\\begin{equation}\n\\E{(y_{t}-\\hat{y}_{t})^{2}}=\\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}\\left|\\Gamma_s(\\omega)-{\\Gamma_\\theta}(\\omega)\\right|^{2}h(\\omega)\\ud\\omega=\\frac{1}{2\\pi}\\times2\\times\\int_{0}^{\\pi}\\left|\\Gamma_s(\\omega)-{\\Gamma_\\theta}(\\omega)\\right|^{2}h(\\omega)\\ud\\omega\n\\tag{7.1}\n\\end{equation}\\]\nequality can also generalized non-stationary integrated process (example imposing cointegration signals using pseudo-spectral density, see Wildi et Mcelroy (2013)).:\n\\[\\begin{align}\n\\left|\\Gamma_s(\\omega)-\\Gamma_\\theta(\\omega)\\right|^{2} & =\\rho_s(\\omega)^{2}+\\rho_\\theta(\\omega)^{2}+2\\rho_s(\\lambda)\\rho_\\theta(\\lambda)\\left(1-\\cos(\\varphi_s(\\omega)-\\varphi_\\theta(\\omega)\\right) \\nonumber\\\\\n & =\\left(\\rho_s(\\omega)-\\rho_\\theta(\\omega)\\right)^{2}+4\\rho_s(\\lambda)\\rho_\\theta(\\lambda)\\sin^{2}\\left(\\frac{\\varphi_s(\\omega)-\\varphi_\\theta(\\omega)}{2}\\right)\n \\tag{7.2}\n\\end{align}\\]interval \\([0,\\pi]\\) splitted two: pass-band \\([0,\\omega_1]\\) (frequency interval contains target signal) stop-band \\([\\omega_1,\\pi]\\).mean square error defined equation (7.1) can decomposed additively four quantities:\n\\[\\begin{align*}\nAccuracy =A_w&= 2\\int_0^{\\omega_1}\\left(\\rho_s(\\omega)-\\rho_\\theta(\\omega)\\right)^{2}h(\\omega)\\ud\\omega\\\\\nTimeliness =T_w&= 8\\int_0^{\\omega_1}\\rho_s(\\lambda)\\rho_\\theta(\\lambda)\\sin^{2}\\left(\\frac{\\varphi_\\theta(\\omega)}{2}\\right)h(\\omega)\\ud\\omega\\\\\nSmoothness =S_w&= 2\\int_{\\omega_1}^\\pi\\left(\\rho_s(\\omega)^{2}-\\rho_\\theta(\\omega)\\right)^{2}h(\\omega)\\ud\\omega\\\\\nResidual =R_w&= 8\\int_{\\omega_1}^\\pi\\rho_s(\\lambda)\\rho_\\theta(\\lambda)\\sin^{2}\\left(\\frac{\\varphi_\\theta(\\omega)}{2}\\right)h(\\omega)\\ud\\omega\\\\\n\\end{align*}\\]coherent definitions sections, formulas four criteria slightly differ ones defined Wildi et McElroy (2019):paper interval integrations \\([0,\\pi]\\) rather \\([-\\pi;\\pi]\\) (integrals multiplied 2 functions even);paper interval integrations \\([0,\\pi]\\) rather \\([-\\pi;\\pi]\\) (integrals multiplied 2 functions even);pass-band interval defined frequency interval contains target signals whereas Wildi et McElroy (2019) depends gain function symmetric filer (pass-band\\(=\\{\\omega |\\rho_s(\\omega)\\geq 0.5\\}\\)).pass-band interval defined frequency interval contains target signals whereas Wildi et McElroy (2019) depends gain function symmetric filer (pass-band\\(=\\{\\omega |\\rho_s(\\omega)\\geq 0.5\\}\\)).general, residual \\(R_w\\) small since \\(\\rho_s(\\omega)\\rho_\\theta(\\omega)\\) close 0 stop-band.\nMoreover, user priorities rarely concerned time-shift properties components stop-band.\n’s , derive linear filters residual taken account; ’s authors suggest minimizing weighted sum first three indicators:\n\\[\n\\mathcal{M}(\\vartheta_{1},\\vartheta_{2})=\\vartheta_{1}T_w(\\theta)+\\vartheta_{2}S_w(\\theta)+(1-\\vartheta_{1}-\\vartheta_{2})A_w(\\theta)\n\\]\nOne drawbacks method guarantee unique solution.paper focus non-parametric approaches derive linear filters.\n’s approach considered.\nHowever, decomposition mean squared error gave useful indicators compare linear filters , contrary one presented section 4.2, values can easily interpreted compared .criteria don’t depend data (one defined table 3.1), take symmetric filter Henderson filter fix spectral density one random walk:\n\\[\nh_{RW}(x)=\\frac{1}{2(1-\\cos(x))}\n\\]Data-dependent filtersAdvantages:values different criteria can compared: weights can easily interpreted.Drawbacks:Data-dependent filter: depends symmetric filter, data date estimation.Data-dependent filter: depends symmetric filter, data date estimation.optimization problems might occur (several minimum, etc.).optimization problems might occur (several minimum, etc.).also use non-parametric approach build asymmetric using \\(h_{RW}\\) spectral density series.\nHowever, due lack time, option considered study.","code":""},{"path":"sec:comparison.html","id":"sec:comparison","chapter":"Chapitre 8 Comparison des différentes méthodes","heading":"Chapitre 8 Comparison des différentes méthodes","text":"","code":""},{"path":"sec:comparison.html","id":"comparison-with-the-fst-approach","chapter":"Chapitre 8 Comparison des différentes méthodes","heading":"8.1 Comparison with the FST approach","text":"L’approche FST fournit unThe FST approach provides useful tool validate method construction linear filter.\nIndeed, linear filter can considered suboptimal can find set weight FST approach gives better results (terms fidelity, smoothness timeliness) (higher) polynomial constraints.\\(h=6\\) (13-term symmetric filter), RKHS filters find :imposing asymmetric filters preserve constants, FST approach gives better results values \\(q\\) filters computed \\(b_{q,G}\\) \\(b_{q,\\gamma}\\), \\(q\\leq 3\\) one computed \\(b_{q,\\varphi}\\).imposing asymmetric filters preserve constants, FST approach gives better results values \\(q\\) filters computed \\(b_{q,G}\\) \\(b_{q,\\gamma}\\), \\(q\\leq 3\\) one computed \\(b_{q,\\varphi}\\).imposing asymmetric filters preserve linear trends, FST approach gives better results \\(q\\[2,5]\\) filters computed \\(b_{q,G}\\) \\(b_{q,\\gamma}\\), \\(q\\[2,4]\\) one computed \\(b_{q,\\varphi}\\).imposing asymmetric filters preserve linear trends, FST approach gives better results \\(q\\[2,5]\\) filters computed \\(b_{q,G}\\) \\(b_{q,\\gamma}\\), \\(q\\[2,4]\\) one computed \\(b_{q,\\varphi}\\). Therefore, RKHS approach seems better approach build asymmetric filters apply monthly regular data (.e.: data usually use \\(h=6\\)).local polynomial filters find :imposing asymmetric filters preserve constants, FST approach gives better results LC method values \\(q\\).\nAdding timeliness criterion allows better results FST approach \\(q=0,1\\).imposing asymmetric filters preserve constants, FST approach gives better results LC method values \\(q\\).\nAdding timeliness criterion allows better results FST approach \\(q=0,1\\).imposing asymmetric filters preserve linear trends, FST approach gives better results \\(q\\[2,5]\\) LC QL methods.\nAdding timeliness criterion doesn’t change results.imposing asymmetric filters preserve linear trends, FST approach gives better results \\(q\\[2,5]\\) LC QL methods.\nAdding timeliness criterion doesn’t change results. terms quality criteria, local polynomial seems perform better real-time one-period ahead filters (\\(q=0,1\\)).comparisons can seen online application available https://aqlt.shinyapps.io/FSTfilters/.","code":""},{"path":"sec:comparison.html","id":"illustration-with-an-example","chapter":"Chapitre 8 Comparison des différentes méthodes","heading":"8.2 Illustration with an example","text":"section compare different asymmetric filters applying French industrial production index manufacture motor vehicles, trailers semi-trailers14.\nsimplicity, FST approach show extreme filter example minimizing phase shift putting large weight timeliness (\\(\\alpha = 0\\), \\(\\beta = 1\\), \\(\\gamma = 1000\\)).\nlocal polynomial filters, Henderson kernel used \\(R=3.5\\).\nfigure 8.1 shows results around turning point (economic crisis 2009) “normal cycles” (2000 2008), \\(q=0\\) (real-time estimates) \\(q=3\\) (three future available observations).\nFigure 8.1 : Application asymmetric filters French industrial production index manufacture motor vehicles, trailers semi-trailers.\n\nNote: final trend one computed symmetric 13-terms Henderson filter.\nexample, terms time lag detect turning point February 200915, DAF worst filter since introduces delay one month \\(q=3\\).\nLC, QL, \\(b_{q,\\gamma}\\) filters introduce delay real-time filter (\\(q=0\\)) two months QL filter one month others.\nSurprisingly, FST filter minimize phase shift introduce time lag \\(q=0\\) \\(q=1\\) (upturn detected earlier), whereas \\(b_{q,\\phi}\\) doesn’t introduce time lag.results differ one obtained real-time economic crisis.\nIndeed, take time series already seasonally adjusted overall period, whereas real-time estimates, seasonal adjustment process also introduce time lag.terms revision error, DAF filters also give worst results.\nreal-time estimates (\\(q=0\\)), LC RKHS filters minimize revision 2000 2008 data available methods (except DAF filters) give similar results.\nexcepted, QL filter minimizes revision around turning point 2009 since filter preserve polynomial trends degree 1.\nsmall values \\(q\\), \\(b_{q,\\phi}\\) filter distorts less polynomial trends degree 1 2 RKHS filters (\\(b_l\\) \\(b_q\\) statistics lower, table 6.1): explains performs better around turning point.simple example generalize properties different asymmetric filters, gives leads extend study:minimize timeliness doesn’t imply earlier detection turning points: investigations timeliness criterion done;minimize timeliness doesn’t imply earlier detection turning points: investigations timeliness criterion done;accept bias polynomial trends preservation can lead asymmetric filters performs better term turning points detection;accept bias polynomial trends preservation can lead asymmetric filters performs better term turning points detection;almost filters, time lag detect turning points two future observations available, different methods gives similar results.\nTherefore, pointless optimize asymmetric filters much future observations available.almost filters, time lag detect turning points two future observations available, different methods gives similar results.\nTherefore, pointless optimize asymmetric filters much future observations available.Moreover, example also illustrates different methods perform better DAF filters.\nreal-time estimates methods use filter easily improved.\nexample case STL (Seasonal-Trend decomposition based Loess) seasonal adjustment method proposed Cleveland et al. (1990).","code":""},{"path":"comparaison-avec-le-filtre-fst.html","id":"comparaison-avec-le-filtre-fst","chapter":"A Comparaison avec le filtre FST","heading":"A Comparaison avec le filtre FST","text":"Cette annexe synthétise les cas où il est possible de trouver un ensemble de poids tels que les filtres FST ont de meilleurs propriété (en termes de fidelity, smoothness et timeliness) que les filtres polynomiaux ou RKHS avec les mêmes contraintes sur la préservation des polynômes (ou des contraintes supérieures).","code":""},{"path":"comparaison-avec-le-filtre-fst.html","id":"filtres-polynomiaux-locaux","chapter":"A Comparaison avec le filtre FST","heading":"A.1 Filtres polynomiaux locaux","text":"\nFigure .1 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre Linear-Constant (LC) sous contrainte de préservation des constantes et pour \\(h=6\\), \\(/C=3,5\\).\n\nFigure .2 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre Linear-Constant (LC) sous contrainte de préservation des droites et pour \\(h=6\\), \\(/C=3,5\\).\n\nFigure .3 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre Linear-Constant (LC) sous contrainte de préservation des constantes et pour \\(h=11\\), \\(/C=3,5\\).\n\nFigure .4 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre Linear-Constant (LC) sous contrainte de préservation des droites et pour \\(h=11\\), \\(/C=3,5\\).\n","code":""},{"path":"comparaison-avec-le-filtre-fst.html","id":"filtres-rkhs","chapter":"A Comparaison avec le filtre FST","heading":"A.2 Filtres RKHS","text":"\nFigure .5 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre \\(b_{q,\\varphi}\\) sous contrainte de préservation des constantes et pour \\(h=6\\).\n\nFigure .6 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre \\(b_{q,\\varphi}\\) sous contrainte de préservation des droites et pour \\(h=6\\).\n\nFigure .7 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre \\(b_{q,\\varphi}\\) sous contrainte de préservation des tendances quadratiques et pour \\(h=6\\).\n\nFigure .8 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre \\(b_{q,\\varphi}\\) sous contrainte de préservation des constantes et pour \\(h=11\\).\n\nFigure .9 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre \\(b_{q,\\varphi}\\) sous contrainte de préservation des droites et pour \\(h=11\\).\n\nFigure .10 : Ensemble des poids pour lesquels la méthode FST donne des filtres de meilleurs qualité que le filtre \\(b_{q,\\varphi}\\) sous contrainte de préservation des tendances quadratiques et pour \\(h=11\\).\n","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
